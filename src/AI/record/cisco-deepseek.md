# 思科：Deepseek安全评估

**原文：[Evaluating Security Risk in DeepSeek - Cisco Blogs](https://blogs.cisco.com/security/evaluating-security-risk-in-deepseek-and-other-frontier-reasoning-models)**

> 这项研究由思科旗下 Robust Intelligence 的 AI 安全研究员，与宾夕法尼亚大学的 Yaron Singer、Amin Karbasi、Paul Kassianik、Mahdi Sabbaghi、Hamed Hassani 和 George Pappas 等学者紧密合作完成。

## 概要

本文调查了中国 AI 创业公司 DeepSeek 新推出的前沿推理模型 **DeepSeek R1** 的漏洞。该模型以其先进的推理能力和高性价比的训练方法引起全球关注。虽然其性能可与 **OpenAI o1** 等顶尖模型媲美，但我们的安全评估揭示了其 **严重的安全缺陷**。

我们的团队运用 **算法越狱技术**，对 DeepSeek R1 进行了 **自动化攻击测试**，使用了来自 **HarmBench 数据集** 的 50 个随机提示。这些提示涵盖了 **六类有害行为**，包括网络犯罪、虚假信息、非法活动和一般危害。

结果令人震惊：**DeepSeek R1 的攻击成功率高达 100%**，这意味着它未能阻止任何一个有害提示。这与其他领先模型形成鲜明对比，后者至少表现出一定的抵抗能力。

我们的研究结果表明，DeepSeek 声称的高性价比训练方法，包括 **强化学习**、**思维链自评估** 和 **知识蒸馏**，可能损害了其安全机制。与其他前沿模型相比，DeepSeek R1 缺乏有效的安全防护，极易遭受 **算法越狱** 和潜在的滥用。

我们将发布 **后续报告**，详细介绍 **推理模型算法越狱** 方面的进展。我们的研究强调，AI 开发迫切需要进行 **严格的安全评估**，以确保效率和推理能力的突破不会以牺牲安全为代价。这也再次强调了企业使用 **第三方安全防护** 的重要性，以便为 AI 应用提供持续、可靠的安全保护。

:::info 译者加
用户：有这种好事？
:::

## 引言

过去一周，新闻头条几乎都被中国 AI 创业公司 DeepSeek 推出的新型推理模型 DeepSeek R1 占据。这款模型及其在基准测试中惊人的表现，不仅吸引了 AI 界的目光，也引发了全球关注。

关于 DeepSeek R1 的媒体报道铺天盖地，都在分析其影响以及对全球 AI 创新的意义。然而，鲜少有人讨论这款模型的安全性。因此，我们决定采用类似于 [AI Defense](https://www.cisco.com/site/us/en/products/security/ai-defense/index.html) 算法漏洞测试的方法，对 DeepSeek R1 进行测试，以深入了解其安全状况。

本文将解答三个主要问题：DeepSeek R1 为何重要？为何必须了解 DeepSeek R1 的漏洞？与其它前沿模型相比，DeepSeek R1 的安全性如何？

## DeepSeek R1 是什么？为何重要？

尽管近年来成本效益和计算能力有所提升，但目前顶尖的 AI 模型仍需耗费数亿美元和庞大的计算资源进行构建和训练。DeepSeek 的模型却以据称远低于行业平均水平的资源，实现了与领先前沿模型相媲美的效果。

DeepSeek 近期发布的模型，特别是 DeepSeek R1-Zero（据称完全通过强化学习训练）和 DeepSeek R1（使用监督学习优化 R1-Zero），都突显了其对开发具备先进推理能力的大语言模型 (LLM) 的重视。[他们的研究](https://arxiv.org/abs/2501.12948) 表明，DeepSeek R1 在数学、编程和科学推理等任务上的性能，可与 OpenAI o1 模型相提并论，甚至超越 Claude 3.5 Sonnet 和 ChatGPT-4o。最值得注意的是，据称 DeepSeek R1 的训练成本约为 600 万美元，仅为 OpenAI 等公司数十亿美元投入的一小部分。

DeepSeek 模型训练方式的不同之处，可归纳为以下三个原则：

*   思维链：模型能够自评估性能。
*   强化学习：模型能够自我引导。
*   知识蒸馏：能够从大型原始模型（6710 亿参数）中，开发出更小（15 亿至 700 亿参数）的模型，以扩大应用范围。

思维链提示使 AI 模型能够像人类解数学题时展示步骤一样，将复杂问题分解为更小的步骤。这种方法结合了“草稿纸”功能，模型可以在最终答案之外单独进行中间计算。如果模型在此过程中出错，它可以回溯到之前的正确步骤，尝试不同的方法。

此外，强化学习技术奖励模型生成准确的中间步骤，而不仅仅是正确的最终答案。这些方法显著提高了 AI 在需要详细推理的复杂问题上的表现。

知识蒸馏是一种创建更小、更高效模型的技术，这些模型保留了大型模型的大部分能力。其工作原理是使用大型“教师”模型来训练较小的“学生”模型。通过这个过程，学生模型学会复制教师模型在特定任务上的问题解决能力，同时减少了对计算资源的需求。

DeepSeek 将思维链提示和奖励建模与知识蒸馏相结合，创造出的模型在推理任务中显著优于传统的大语言模型 (LLM)，同时保持了较高的运行效率。

## 为何必须了解 DeepSeek 的漏洞？

DeepSeek 背后的理念是全新的。自 OpenAI 的 o1 模型问世以来，模型提供商一直专注于构建具备推理能力的模型。自 o1 之后，大语言模型 (LLM) 已经能够通过与用户的持续互动，以自适应的方式完成任务。然而，DeepSeek R1 团队证明，即使不依赖昂贵的人工标注数据集或庞大的计算资源，也能实现高性能。

毫无疑问，DeepSeek 模型的性能对 AI 领域产生了巨大影响。我们不能只关注性能，还必须了解 DeepSeek 及其新的推理模式，是否在安全方面存在重大妥协。

## DeepSeek 与其他前沿模型相比，安全性如何？

### 方法

我们对几种流行的前沿模型以及两个推理模型：DeepSeek R1 和 OpenAI O1-preview 进行了安全测试。

为了评估这些模型，我们对来自流行的 HarmBench 基准测试的 50 个均匀采样的提示，运行了自动越狱算法。[HarmBench](https://arxiv.org/abs/2402.04249) 基准测试总共包含 400 种行为，涵盖 7 个危害类别，包括网络犯罪、虚假信息、非法活动和一般危害。

我们的关键指标是攻击成功率 (ASR)，它衡量的是找到越狱行为的百分比。这是越狱场景中常用的标准指标，我们在此评估中也采用了该指标。

我们以温度 0 对目标模型进行采样：这是最保守的设置。这保证了我们生成的攻击的可重复性和保真度。

我们使用了自动方法进行拒绝检测，并进行了人工监督以验证越狱。

### 结果

据称，DeepSeek R1 的训练预算远低于其他前沿模型提供商的投入。然而，这带来了另一种代价：安全。

我们的研究团队成功地对 DeepSeek R1 进行了越狱，攻击成功率高达 100%。这意味着在 HarmBench 数据集中，没有一个提示未能从 DeepSeek R1 获得肯定回答。这与 o1 等其他前沿模型形成对比，后者通过模型安全防护机制，阻止了大部分对抗性攻击。

下图显示了我们的总体结果。

<Cisco1/>

> 显示流行大语言模型攻击成功率的图表，DeepSeek-R1 的成功率为 100%，Llama-3.1-405B 为 96%，GPT-4o 为 86%，Gemini-1.5-pro 为 64%，Claude-3.5-Sonnet 为 36%，O1-preview 为 26%

下表更详细地展示了每个模型对不同危害类别提示的响应情况。

<Cisco2/>

> 显示每个模型和类别越狱百分比的表格。Deepseek 在所有类别中的越狱百分比均为 100%，类别包括化学生物、网络犯罪入侵、骚扰霸凌、有害、非法和虚假信息

**关于算法越狱和推理的说明：** 此分析由 Robust Intelligence（现为思科一部分）的先进 AI 研究团队，与宾夕法尼亚大学的研究人员合作完成。本次评估的总成本不到 50 美元，采用的是完全算法化的验证方法，类似于我们在 AI Defense 产品中使用的方法。此外，这种算法方法应用于推理模型，其能力超越了我们去年在 [Tree of Attack with Pruning (TAP)](https://arxiv.org/abs/2312.02119) 研究中提出的能力。在后续文章中，我们将更详细地讨论算法越狱推理模型的这项新能力。

<script setup>
    import Cisco1 from "./Cisco1.vue";
    import Cisco2 from "./cisco2.vue";
</script>
